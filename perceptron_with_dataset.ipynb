{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVLP54jooZ9P"
      },
      "outputs": [],
      "source": [
        "Nice — let’s solve both parts quickly.\n",
        "\n",
        "## (a) Perceptron network design\n",
        "\n",
        "You have four classes and 2-D inputs. A standard way is to use **two binary perceptrons** whose outputs `(Y1,Y2)` are interpreted as a 2-bit code:\n",
        "\n",
        "* Class 1 → `(0,0)` for points `{[1,1], [1,2]}`\n",
        "* Class 2 → `(0,1)` for points `{[2,-1], [2,0]}`\n",
        "* Class 3 → `(1,0)` for points `{[-1,2], [-2,1]}`\n",
        "* Class 4 → `(1,1)` for points `{[-1,-1], [-2,-2]}`\n",
        "\n",
        "So the network is simply two perceptron units (each with its own weights and bias) taking the two inputs `x1,x2` and producing `y1` and `y2`. Each perceptron computes:\n",
        "[\n",
        "y = \\text{step}(w_1 x_1 + w_2 x_2 + b),\\quad \\text{step}(z)=\\begin{cases}1 & z>0\\0 & z\\le 0\\end{cases}\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "## (b) Training with perceptron learning rule (given table)\n",
        "\n",
        "**Given:**\n",
        "\n",
        "* Training samples (in table order):\n",
        "\n",
        "| x1 | x2 | y1 | y2 |\n",
        "| -- | -- | -- | -- |\n",
        "| 1  | 1  | 0  | 0  |\n",
        "| 1  | 2  | 0  | 0  |\n",
        "| 2  | -1 | 0  | 1  |\n",
        "| 2  | 0  | 0  | 1  |\n",
        "| -1 | 2  | 1  | 0  |\n",
        "| -2 | 1  | 1  | 0  |\n",
        "| -1 | -1 | 1  | 1  |\n",
        "| -2 | -2 | 1  | 1  |\n",
        "\n",
        "* Initial weights and biases:\n",
        "\n",
        "  * For perceptron producing `y1`: (w^{(1)}=[W_{11},W_{12}]=[1,0]), bias (b_1=1)\n",
        "  * For perceptron producing `y2`: (w^{(2)}=[W_{21},W_{22}]=[0,1]), bias (b_2=1)\n",
        "\n",
        "* Learning rule (learning rate = 1):\n",
        "\n",
        "  * If output (o) and target (t): ( \\Delta = t-o)\n",
        "  * (w \\leftarrow w + \\Delta \\cdot x)\n",
        "  * (b \\leftarrow b + \\Delta)\n",
        "\n",
        "I ran the perceptron learning loop through the samples (repeating epochs in the same order) until no weight changed. The network **converged after 4 epochs**. The final parameters are:\n",
        "\n",
        "* **Perceptron 1 (Y1):**\n",
        "\n",
        "  * Final weights (w^{(1)} = [-2,;0])\n",
        "  * Final bias (b_1 = -1)\n",
        "\n",
        "* **Perceptron 2 (Y2):**\n",
        "\n",
        "  * Final weights (w^{(2)} = [1,;-3])\n",
        "  * Final bias (b_2 = 0)\n",
        "\n",
        "### Verification (classification on training set)\n",
        "\n",
        "Compute activations (a_i = w^{(i)}\\cdot x + b_i) and apply step:\n",
        "\n",
        "All training points produce the desired outputs `(Y1,Y2)`:\n",
        "\n",
        "* (1,1) → (0,0)\n",
        "* (1,2) → (0,0)\n",
        "* (2,-1) → (0,1)\n",
        "* (2,0) → (0,1)\n",
        "* (-1,2) → (1,0)\n",
        "* (-2,1) → (1,0)\n",
        "* (-1,-1) → (1,1)\n",
        "* (-2,-2) → (1,1)\n",
        "\n",
        "---\n",
        "\n",
        "## Short summary\n",
        "\n",
        "* Network: two perceptrons, each maps 2-D input to binary output bit.\n",
        "* Trained with perceptron rule (lr=1) on given 8 samples.\n",
        "* **Final weights & biases:**\n",
        "\n",
        "  * (w^{(1)}=[-2,,0],; b_1=-1)\n",
        "  * (w^{(2)}=[1,,-3],; b_2=0)\n",
        "\n",
        "If you want, I can:\n",
        "\n",
        "* show the full update trace epoch-by-epoch,\n",
        "* draw the decision boundaries in the plane,\n",
        "* or re-run with a different initialization / learning rate.\n"
      ]
    }
  ]
}